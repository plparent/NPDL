{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux à convolution\n",
    "\n",
    "Dans ce notebook, nous crérons un réseau à convolution.  Mais avant de commencer, assurez-vous d'avoir bien compris les notebooks sur la **convolution**, la **batch-norm** et le **dropout**.\n",
    "\n",
    "Ici, nous utiliserons pour l'essentiel le code dans les fichiers suivants :\n",
    "\n",
    "    model/Model.py\n",
    "    layers/Conv.py \n",
    "    utils/model_loss.py\n",
    "    layers/MaxPool.py\n",
    "    \n",
    "Comme au tp1, la classe **Model** \"crée\" un réseau de neurones en ajoutant successivement des couches et une fonction de perte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, channels, input_size, input_size)\n",
    "    y = np.array([0, 1, 2, 1, 2])\n",
    "    return X, y\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cicles dev\n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    X_train = X_train.transpose(0, 3, 1, 2)\n",
    "    X_test = X_test.transpose(0, 3, 1, 2)\n",
    "    X_val = X_val.transpose(0, 3, 1, 2)\n",
    "    X_dev = X_dev.transpose(0, 3, 1, 2)\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle simple\n",
    "\n",
    "Commençons avec un modèle très simple à une couche cachée.  Pour la fonction:\n",
    "\n",
    "    cross_entropy_loss\n",
    "    \n",
    "vous pouvez récupérer votre code du tp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Model import Model\n",
    "from layers.Conv import Conv2DNaive\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "\n",
    "num_filters = 3\n",
    "filter_size = 4\n",
    "channels = 1\n",
    "num_inputs = 5\n",
    "input_size = 4\n",
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Model()\n",
    "layer = Conv2DNaive(num_filters, filter_size=filter_size, channels=channels, \n",
    "                        stride=stride, padding=padding, weight_scale=1e-1)\n",
    "model.add(layer)\n",
    "model.add_loss(cross_entropy_loss_npdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Récupérez le code \"naïf\" de la convolution que vous avez fait dans   #\n",
    "#   le notebook sur la convolution et le mettre dans la fonction \"forward\"   #\n",
    "#   de la classe Conv2DNaive dans le fichier Conv.py.                        #\n",
    "#   S'il n'y a pas de bug, le test suivant devrait passer                    #\n",
    "##############################################################################\n",
    "X, y = create_toy_data()\n",
    "scores = model.forward_npdl(X).reshape(num_inputs, num_filters)\n",
    "correct_scores = np.asarray([[2.26107191, 1.90356006, -10.7996371 ],\n",
    "                             [-4.03277981, 6.72254124, 0.36232013],\n",
    "                             [-0.33115169, 3.30740614, 8.24078811],\n",
    "                             [-0.78558615, 3.5691293, -5.41204248],\n",
    "                             [7.16759091, -7.01187203, -4.51633761]])\n",
    "\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your scores and correct scores: ', np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dScores, _ = model.calculate_loss(scores, y, 0.1)\n",
    "correct_loss = 2.47808382744825\n",
    "\n",
    "# on devrait obtenir une erreur  inférieure à environ 1e-12.\n",
    "print('Loss: ', loss)\n",
    "print('Correct loss: ', correct_loss)\n",
    "print('Difference between your loss and correct loss: ', np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode backward (rétro-propagation) de la classe de  #\n",
    "# couche Conv2DNaive.                                                        #\n",
    "##############################################################################\n",
    "\n",
    "_ = model.backward_npdl(dScores.reshape(5, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gradients import evaluate_numerical_gradient\n",
    "\n",
    "# Retourne l'erreur relative maximale des matrices de gradients passées en paramètre.\n",
    "# Pour chaque paramètre, l'erreur relative devrait être inférieure à environ 1e-8.\n",
    "def rel_error(x, y):\n",
    "    rel = np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y)))\n",
    "    return np.max(rel)\n",
    "\n",
    "gradients = model.gradients()\n",
    "model_params = model.parameters()\n",
    "\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model, layer_name, param_name, reg=0.1)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        \n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de différentes implantations de la convolution\n",
    "\n",
    "    version naive\n",
    "    version \"matricée\"\n",
    "    version \"optimisée\"\n",
    "\n",
    "La version optimisée implique l'utilisation de code **\"cython\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Model import Model\n",
    "from layers.Conv import Conv2DNaive, Conv2DMat, Conv2DCython\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "\n",
    "num_filters = 3\n",
    "filter_size = 4\n",
    "channels = 1\n",
    "\n",
    "num_inputs = 5\n",
    "input_size = 4\n",
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "def create_toy_model():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    layer = Conv2DNaive(num_filters, filter_size=filter_size, channels=channels, \n",
    "                        stride=stride, padding=padding, weight_scale=1e-1)\n",
    "    model.add(layer)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n",
    "\n",
    "def create_toy_model_mat():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    layer = Conv2DMat(num_filters, filter_size=filter_size, channels=channels, \n",
    "                      stride=stride, padding=padding, weight_scale=1e-1)\n",
    "    model.add(layer)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n",
    "\n",
    "def create_toy_model_fast():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    layer = Conv2DCython(num_filters, filter_size=filter_size, channels=channels, \n",
    "                         stride=stride, padding=padding, weight_scale=1e-1)\n",
    "    model.add(layer)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n",
    "\n",
    "model_naive = create_toy_model()\n",
    "model_mat = create_toy_model_mat()\n",
    "model_fast = create_toy_model_fast()\n",
    "X, y = create_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">CNN avec matricisation des entrées</font>\n",
    "\n",
    "Ici le code \"forward\" et \"backward\" de la convolution \"matricée\" vous est fourni... à un détail prêt:  **il manque dans les 2 cas, la fonction d'activation**.  À vous de l'ajouter à ces deux fonctions (c.f. classe **Conv2DMat**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode forward (propagation avant) de la classe de   #\n",
    "# couche Conv2DMat (convolution matricisée).                                 #\n",
    "##############################################################################\n",
    "\n",
    "scores = model_mat.forward_npdl(X).reshape(num_inputs, num_filters)\n",
    "correct_scores = np.asarray([[2.26107191, 1.90356006, -10.7996371 ],\n",
    "                             [-4.03277981, 6.72254124, 0.36232013],\n",
    "                             [-0.33115169, 3.30740614, 8.24078811],\n",
    "                             [-0.78558615, 3.5691293, -5.41204248],\n",
    "                             [7.16759091, -7.01187203, -4.51633761]])\n",
    "\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your scores and correct scores: ', np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dScores_mat, _ = model_mat.calculate_loss(scores, y, 0.1)\n",
    "correct_loss = 2.47808382744825\n",
    "\n",
    "# on devrait obtenir une erreur  inférieure à environ 1e-12.\n",
    "print('Loss: ', loss)\n",
    "print('Correct loss: ', correct_loss)\n",
    "print('Difference between your loss and correct loss: ', np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode backward (rétro-propagation) de la classe de  #\n",
    "# couche Conv2DMat.                                                          #\n",
    "##############################################################################\n",
    "\n",
    "_ = model_mat.backward_npdl(dScores_mat.reshape(5, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = model_mat.gradients()\n",
    "model_params = model_mat.parameters()\n",
    "\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model_mat, layer_name, param_name, reg=0.1)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        \n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">CNN vectorisé, utilise cython</font>\n",
    "\n",
    "Ici le code \"forward\" et \"backward\" de la convolution \"cythonisée\" vous est fourni... à un détail prêt: **il manque dans les 2 cas, la fonction d'activation**.  À vous de l'ajouter à ces deux fonctions (c.f. la classe **Conv2DCython**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode forward (propagation avant) de la classe de   #\n",
    "# couche Conv2DCython.                                                       #\n",
    "##############################################################################\n",
    "\n",
    "scores = model_fast.forward_npdl(X).reshape(num_inputs, num_filters)\n",
    "correct_scores = np.asarray([[2.26107191, 1.90356006, -10.7996371 ],\n",
    "                             [-4.03277981, 6.72254124, 0.36232013],\n",
    "                             [-0.33115169, 3.30740614, 8.24078811],\n",
    "                             [-0.78558615, 3.5691293, -5.41204248],\n",
    "                             [7.16759091, -7.01187203, -4.51633761]])\n",
    "\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your scores and correct scores: ', np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dScores_fast, _ = model_fast.calculate_loss(scores, y, 0.1)\n",
    "correct_loss = 2.47808382744825\n",
    "\n",
    "# on devrait obtenir une erreur  inférieure à environ 1e-12.\n",
    "print('Loss: ', loss)\n",
    "print('Correct loss: ', correct_loss)\n",
    "print('Difference between your loss and correct loss: ', np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode backward (rétro-propagation) de la classe de  #\n",
    "# couche Conv2DCython.                                                       #\n",
    "##############################################################################\n",
    "\n",
    "_ = model_fast.backward_npdl(dScores_fast.reshape(5, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = model_fast.gradients()\n",
    "model_params = model_fast.parameters()\n",
    "\n",
    "# L'erreur devrait être inférieure à 1e-5\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model_fast, layer_name, param_name, reg=0.1)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        \n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Comparaison des performances</font>\n",
    "\n",
    "Maintenant, voyons à quel point le type d'implantation d'une convolution peut avoir un impact sur la rapidité d'exécution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Forward</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = create_toy_model()\n",
    "model_mat = create_toy_model_mat()\n",
    "model_fast = create_toy_model_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"version naive...\")\n",
    "%timeit model_naive.forward_npdl(X)\n",
    "print(\"version matricée...\")\n",
    "%timeit model_mat.forward_npdl(X)\n",
    "print(\"version rapide cythonisée...\")\n",
    "%timeit model_fast.forward_npdl(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Rétro-propagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = create_toy_model()\n",
    "model_mat = create_toy_model_mat()\n",
    "model_fast = create_toy_model_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = model_naive.forward_npdl(X).reshape(num_inputs, num_filters)\n",
    "loss_naive, dScores_naive, _ = model_naive.calculate_loss(scores, y, 0.1)\n",
    "\n",
    "scores_mat = model_mat.forward_npdl(X).reshape(num_inputs, num_filters)\n",
    "loss_mat, dScores_mat, _ = model_fast.calculate_loss(scores, y, 0.1)\n",
    "\n",
    "scores_fast = model_fast.forward_npdl(X).reshape(num_inputs, num_filters)\n",
    "loss_fast, dScores_fast, _ = model_fast.calculate_loss(scores, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En principe, les loss et des gradients devraient être les mêmes\n",
    "\n",
    "if np.abs(loss_naive - loss_mat) + \\\n",
    "    np.abs(loss_naive - loss_fast) + \\\n",
    "    np.abs(loss_mat - loss_fast) > 1e-6:\n",
    "    print(\"Erreur!\")\n",
    "else:\n",
    "    print(\"Loss bonne!\")    \n",
    "\n",
    "if (np.abs(dScores_mat - dScores_naive)).flatten().mean() + \\\n",
    "    (np.abs(dScores_fast - dScores_naive)).flatten().mean() + \\\n",
    "    (np.abs(dScores_mat - dScores_fast)).flatten().mean() > 1e-6 :\n",
    "    print(\"Erreur!\")\n",
    "else:\n",
    "    print(\"Gradients bons!\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"version naive...\")\n",
    "%timeit _ = model.backward_npdl(dScores_naive.reshape(5, 3, 1, 1))\n",
    "print(\"version matricée...\")\n",
    "%timeit _ = model_mat.backward_npdl(dScores_mat.reshape(5, 3, 1, 1))\n",
    "print(\"version rapide cythonisée...\")\n",
    "%timeit _ = model_fast.backward_npdl(dScores_fast.reshape(5, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">CNN à 2 couches</font>\n",
    "\n",
    "Ici nous testerons des réseaux à 2 couches convolutives.  En principe, si le code des dernières cellules fonctionne, le code des prochaines cellules devrait fonctionner!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Model import Model\n",
    "from layers.Conv import Conv2DNaive, Conv2DMat, Conv2DCython\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "\n",
    "num_filters = 4\n",
    "filter_size = 3\n",
    "channels = 2\n",
    "\n",
    "num_inputs = 5\n",
    "input_size = 5\n",
    "padding = 1\n",
    "stride = 2\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "def create_toy_model():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    layer0 = Conv2DNaive(num_filters, filter_size=filter_size, channels=channels, \n",
    "                         stride=stride, padding=padding, weight_scale=1e-1)\n",
    "    layer1 = Conv2DNaive(num_classes, filter_size=3, channels=num_filters, \n",
    "                         weight_scale=1e-1)\n",
    "    \n",
    "    model.add(layer0)\n",
    "    model.add(layer1)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n",
    "\n",
    "def create_toy_model_mat():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    layer0 = Conv2DMat(num_filters, filter_size=filter_size, channels=channels, \n",
    "                       stride=stride, padding=padding, weight_scale=1e-1)\n",
    "    layer1 = Conv2DMat(num_classes, filter_size=3, channels=num_filters, \n",
    "                       weight_scale=1e-1)\n",
    "    model.add(layer0)\n",
    "    model.add(layer1)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n",
    "\n",
    "def create_toy_model_fast():\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    layer0 = Conv2DCython(num_filters, filter_size=filter_size, channels=channels, \n",
    "                          stride=stride, padding=padding, weight_scale=1e-1)\n",
    "    layer1 = Conv2DCython(num_classes, filter_size=3, channels=num_filters, \n",
    "                          weight_scale=1e-1)\n",
    "    model.add(layer0)\n",
    "    model.add(layer1)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n",
    "\n",
    "def create_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, channels, input_size, input_size)\n",
    "    y = np.array([0, 1, 2, 1, 2])\n",
    "    return X, y\n",
    "\n",
    "model_naive = create_toy_model()\n",
    "model_mat = create_toy_model_mat()\n",
    "model_fast = create_toy_model_fast()\n",
    "X, y = create_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = model_naive.forward_npdl(X).reshape(num_inputs, num_classes)\n",
    "loss_naive, dScores_naive, _ = model_naive.calculate_loss(scores_naive, y, 0.1)\n",
    "\n",
    "scores_mat = model_mat.forward_npdl(X).reshape(num_inputs, num_classes)\n",
    "loss_mat, dScores_mat, _ = model_mat.calculate_loss(scores_mat, y, 0.1)\n",
    "\n",
    "scores_fast = model_fast.forward_npdl(X).reshape(num_inputs, num_classes)\n",
    "loss_fast, dScores_fast, _ = model_fast.calculate_loss(scores_fast, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En principe, les loss et des gradients devraient être les mêmes\n",
    "\n",
    "if np.abs(loss_naive - loss_mat) + \\\n",
    "    np.abs(loss_naive - loss_fast) + \\\n",
    "    np.abs(loss_mat - loss_fast) > 1e-6:\n",
    "    print(\"Erreur!\")\n",
    "else:\n",
    "    print(\"Loss bonne!\")    \n",
    "\n",
    "if (np.abs(dScores_mat - dScores_naive)).flatten().mean() + \\\n",
    "    (np.abs(dScores_fast - dScores_naive)).flatten().mean() + \\\n",
    "    (np.abs(dScores_mat - dScores_fast)).flatten().mean() > 1e-6 :\n",
    "    print(\"Erreur!\")\n",
    "else:\n",
    "    print(\"Gradients bons!\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model_naive.backward_npdl(dScores_naive.reshape(5, 3, 1, 1))\n",
    "_ = model_mat.backward_npdl(dScores_mat.reshape(5, 3, 1, 1))\n",
    "_ = model_fast.backward_npdl(dScores_fast.reshape(5, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification du gradient pour le modèle naif, devrait être inférieur à 1e-4\n",
    "gradients = model_mat.gradients()\n",
    "model_params = model_mat.parameters()\n",
    "\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model_mat, layer_name, param_name, reg=0.1)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        \n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check pour le modèle matricisé, devrait être inférieur à 1e-4\n",
    "gradients = model_mat.gradients()\n",
    "model_params = model_mat.parameters()\n",
    "\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model_mat, layer_name, param_name, reg=0.1)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        \n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check pour le modèle cythonisé.\n",
    "# Les valeurs peuvent être légèrement différentes de celles\n",
    "# obtenues pour model_naive et model_mat (Effet de bord de Cython).\n",
    "# En autant qu'elles restent inférieures à 1e-4, c'est correct.\n",
    "gradients = model_fast.gradients()\n",
    "model_params = model_fast.parameters()\n",
    "\n",
    "for layer_name, layer_params in model_params.items():\n",
    "    for param_name, _ in layer_params.items():\n",
    "        grad_num = evaluate_numerical_gradient(X, y, model_fast, layer_name, param_name, reg=0.1)\n",
    "        max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "        \n",
    "        print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Comparaison des performances</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Propagation avant</font>\n",
    "\n",
    "les différentes implantation de convolution devraient entraîner divers temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = create_toy_model()\n",
    "model_mat = create_toy_model_mat()\n",
    "model_fast = create_toy_model_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN 2 couches, conv naive\")\n",
    "%timeit model_naive.forward_npdl(X)\n",
    "print(\"CNN 2 couches, conv mat\")\n",
    "%timeit model_mat.forward_npdl(X)\n",
    "print(\"CNN 2 couches, conv fast\")\n",
    "%timeit model_fast.forward_npdl(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Rétro-propagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = model_naive.forward_npdl(X).reshape(num_inputs, num_classes)\n",
    "_, dScores_naive, _ = model_naive.calculate_loss(scores_naive, y, 0.1)\n",
    "\n",
    "scores_mat = model_mat.forward_npdl(X).reshape(num_inputs, num_classes)\n",
    "_, dScores_mat, _ = model_fast.calculate_loss(scores, y, 0.1)\n",
    "\n",
    "scores_fast = model_fast.forward_npdl(X).reshape(num_inputs, num_classes)\n",
    "_, dScores_fast, _ = model_fast.calculate_loss(scores, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN 2 couches, gradient conv naive\")\n",
    "%timeit _ = model_naive.backward_npdl(dScores_naive.reshape(5, 3, 1, 1))\n",
    "print(\"CNN 2 couches, gradient conv mat\")\n",
    "%timeit _ = model_mat.backward_npdl(dScores_mat.reshape(5, 3, 1, 1))\n",
    "print(\"CNN 2 couches, gradient conv fast\")\n",
    "%timeit _ = model_fast.backward_npdl(dScores_fast.reshape(5, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">MaxPool</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Propagation avant</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode forward (propagation avant) de la classe de   #\n",
    "# couche MaxPool2DNaive.                                                     #\n",
    "##############################################################################\n",
    "import numpy as np\n",
    "from layers.MaxPool import MaxPool2DNaive\n",
    "\n",
    "X_shape = (2, 3, 4, 4)\n",
    "X = np.linspace(-0.3, 0.4, num=np.prod(X_shape)).reshape(X_shape)\n",
    "\n",
    "layer = MaxPool2DNaive(pooling_size=(2,2), stride=(2,2))\n",
    "\n",
    "out = layer.forward_npdl(X)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Retourne l'erreur relative maximale des matrices de gradients passées en paramètre.\n",
    "# Pour chaque paramètre, l'erreur relative devrait être inférieure à environ 1e-8.\n",
    "print('difference: ', (out - correct_out).flatten().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Rétro-propagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode backward (rétro-propagation) de la classe de  #\n",
    "# couche MaxPool2DNaive.                                                     #\n",
    "##############################################################################\n",
    "\n",
    "X = np.random.randn(2, 2, 4, 4)\n",
    "\n",
    "layer = MaxPool2DNaive(pooling_size=(2,2), stride=(2,2))\n",
    "\n",
    "out = layer.forward_npdl(X)\n",
    "dX = layer.backward_npdl(out)\n",
    "\n",
    "print(\"Inputs\")\n",
    "print(X)\n",
    "# Devrait retouner le max de chaque convolution\n",
    "print(\"Outputs\")\n",
    "print(out)\n",
    "# Devrait replacer les éléments des sorties au même endroit que X\n",
    "print(\"Gradients\")\n",
    "print(dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">ConvNet à N couches</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les cellules qui suivent, vous devez compléter la fonction \n",
    "\n",
    "    create_Nlayer_cnn(num_filter_layer1, num_filter_layer2, fc_size, weight_scale)\n",
    "    \n",
    "avec au moins **3 couches convolutives**, du *max pooling*, du *dropout* des opérations *batchNorm* et atteindre les performances mentionnées plus loins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.BatchNorm import SpatialBatchNorm\n",
    "from layers.Conv import Conv2DCython\n",
    "from layers.Dense import Dense\n",
    "from layers.Flatten import Flatten\n",
    "from layers.MaxPool import MaxPool2DCython\n",
    "from layers.Dropout import Dropout\n",
    "from model.Model import Model\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "\n",
    "# paramètres de convolution, à modifier au besoin\n",
    "filter_size = 5\n",
    "channels = 3\n",
    "stride = 1\n",
    "p_dropout = 0.1\n",
    "pad = int((filter_size - 1)/2)\n",
    "\n",
    "# paramètres dense\n",
    "num_classes = 10\n",
    "\n",
    "def create_Nlayer_cnn(num_filter_layer1, num_filter_layer2, fc_size, init_weight_scale):\n",
    "    model = Model()\n",
    "    \n",
    "    conv1 = Conv2DCython(num_filter_layer1, filter_size=filter_size, channels=channels, padding=pad, weight_scale=init_weight_scale)\n",
    "    batchnorm1 = SpatialBatchNorm(num_filter_layer1, activation='relu')\n",
    "    dropout1 = Dropout(drop_rate=p_dropout)\n",
    "    maxpool1 = MaxPool2DCython(pooling_size=2, stride=2)\n",
    "\n",
    "    conv2 = Conv2DCython(num_filter_layer2, filter_size=filter_size, channels=num_filter_layer1, padding=pad, weight_scale=init_weight_scale)\n",
    "    batchnorm2 = SpatialBatchNorm(num_filter_layer2, activation='relu')\n",
    "    dropout2 = Dropout(drop_rate=p_dropout)\n",
    "    maxpool2 = MaxPool2DCython(pooling_size=2, stride=2)\n",
    "    \n",
    "    conv_fc1 = Conv2DCython(fc_size, filter_size=8, channels=num_filter_layer2, weight_scale=init_weight_scale, activation='relu')\n",
    "    dropout3 = Dropout(drop_rate=p_dropout)\n",
    "    conv_fc2 = Conv2DCython(num_classes, filter_size=1, channels=fc_size, weight_scale=init_weight_scale)\n",
    "    flatten = Flatten()\n",
    "    \n",
    "    model.add(conv1)\n",
    "    model.add(batchnorm1)\n",
    "    model.add(dropout1)\n",
    "    model.add(maxpool1)\n",
    "\n",
    "    model.add(conv2)\n",
    "    model.add(batchnorm2)\n",
    "    model.add(dropout2)\n",
    "    model.add(maxpool2)\n",
    "\n",
    "    model.add(conv_fc1)\n",
    "    model.add(dropout3)\n",
    "    model.add(conv_fc2)\n",
    "    model.add(flatten)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Validation de la perte</font>\n",
    "\n",
    "En augmentant la régularisation, la loss devrait augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_Nlayer_cnn(32, 16, 50, 1e-2)\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "scores = model.forward_npdl(X)\n",
    "\n",
    "loss, grads, _ = model.calculate_loss(scores, y, reg=0.0)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "loss, grads, _ = model.calculate_loss(scores, y, reg=0.1)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Sur-apprendre sur un petit ensemble de données</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici on s'entraîne sur un petit ensemble d'entraînement afin de s'assurer que le modèle\n",
    "# est capable d'overfitter.\n",
    "from model.Solver import epoch_solver_npdl, Adam\n",
    "\n",
    "N = 100\n",
    "X_train_small = X_train[:N]\n",
    "y_train_small = y_train[:N]\n",
    "\n",
    "model = create_Nlayer_cnn(32, 16, 400, 1e-2)\n",
    "\n",
    "optimizer = Adam(5e-4, model)\n",
    "    \n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_npdl(X_train_small, \n",
    "                                                                          y_train_small,\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          1e-2,\n",
    "                                                                          optimizer,\n",
    "                                                                          batch_size=10,\n",
    "                                                                          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, '-o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy_history, '-o')\n",
    "plt.plot(val_accuracy_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Entraînement complet</font>\n",
    "\n",
    "- Entraînez votre réseau pour 3 epochs.  Vous devriez avoir une justesse en validation d'**au moins 48\\%** lorsqu'entraîné sur 6,000 données.\n",
    "- Entraînez votre réseau pour 1 autre epoch mais sur **100% des données**.  Vous devriez avoir une justesse en validation d'**au moins 58\\%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Solver import epoch_solver_npdl, Adam\n",
    "\n",
    "reg = 1e-2 # à ajuster au besoin\n",
    "lr = 5e-4  # à ajuster au besoin\n",
    "model = create_Nlayer_cnn(32, 16, 400, reg)\n",
    "\n",
    "optimizer = Adam(lr, model)\n",
    "\n",
    "# change back to full X_train y_train for complete dataset\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_npdl(X_train[:6000], \n",
    "                                                                          y_train[:6000],\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          reg,\n",
    "                                                                          optimizer,\n",
    "                                                                          batch_size=100,\n",
    "                                                                          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, '-o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Train : 10,000 images')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy_history, '-o')\n",
    "plt.plot(val_accuracy_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On réentraîne (1 epoch) avec 100% des données d'entraînement.\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_npdl(X_train, \n",
    "                                                                          y_train,\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          reg,\n",
    "                                                                          optimizer,\n",
    "                                                                          batch_size=100,\n",
    "                                                                          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, '-o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Train : 50,000 images')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy_history, '-o')\n",
    "plt.plot(val_accuracy_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des filtres\n",
    "Vous pouvez visualiser les filtres de la première couche du réseau entraîné :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.utils import visualize_as_grid\n",
    "\n",
    "def show_net_weights(model):\n",
    "    W1 = model.parameters()['L0']['W']\n",
    "    W1 = W1.transpose(0, 2, 3, 1)\n",
    "    plt.imshow(visualize_as_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
