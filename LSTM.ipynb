{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers.utils.LSTMCell import LSTMCell\n",
    "from model.Model import Model\n",
    "from utils.model_loss import cross_entropy_loss_npdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_cell():\n",
    "    np.random.seed(0)\n",
    "    weights = {'Wc_i': np.random.rand(3, 5),\n",
    "               'Wu_i': np.random.rand(3, 5),\n",
    "               'Wf_i': np.random.rand(3, 5),\n",
    "               'Wo_i': np.random.rand(3, 5),\n",
    "               'Wc_h': np.random.rand(5, 5),\n",
    "               'Wu_h': np.random.rand(5, 5),\n",
    "               'Wf_h': np.random.rand(5, 5),\n",
    "               'Wo_h': np.random.rand(5, 5)}\n",
    "    biases = {'bc': np.zeros(5),\n",
    "              'bu': np.zeros(5),\n",
    "              'bf': np.zeros(5),\n",
    "              'bo': np.zeros(5)}\n",
    "    np.random.seed(0)\n",
    "    return LSTMCell(3, 5, weights, biases)\n",
    "\n",
    "def create_toy_data():\n",
    "    np.random.seed(1)\n",
    "    return np.random.uniform(-1, 1, (5, 3)), np.random.uniform(-1, 1, (5, 5)), np.random.uniform(-1, 1, (5, 5)), np.array([0, 1, 1, 4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = create_toy_cell()\n",
    "x, h, c, y = create_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_h_next = np.array([[ 0.05432073, -0.0240766, -0.08374854, 0.00076676, -0.14279187],\n",
    "                           [-0.26776613, 0.31730641, -0.21700223, -0.28390231, -0.26996079],\n",
    "                           [-0.13447024, -0.08660263, -0.10022437, -0.03162882, -0.05513277],\n",
    "                           [-0.07544577, 0.17997456, -0.08875356, 0.28115015, 0.28349453],\n",
    "                           [-0.20265419, -0.04393417, 0.15789945, -0.00593344, -0.18382524]])\n",
    "\n",
    "correct_c_next = np.array([[ 0.14534732, -0.12841532, -0.17270371, 0.0028584, -0.33097237],\n",
    "                           [-0.40553132, 0.52289938, -0.46666314, -0.41915372, -0.52302227],\n",
    "                           [-0.5563115, -0.20751267, -0.33762134, -0.14975814, -0.34766355],\n",
    "                           [-0.12754703, 0.35234149, -0.1480342, 0.517628, 0.47883466],\n",
    "                           [-0.30773508, -0.14781391, 0.21382212, -0.01451843, -0.31231354]])\n",
    "\n",
    "c_next, h_next, h_up  = cell.forward_npdl(x, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your h_next:\n",
      "[[ 0.05432073 -0.0240766  -0.08374854  0.00076676 -0.14279187]\n",
      " [-0.26776613  0.31730641 -0.21700223 -0.28390231 -0.26996079]\n",
      " [-0.13447024 -0.08660263 -0.10022437 -0.03162882 -0.05513277]\n",
      " [-0.07544577  0.17997456 -0.08875356  0.28115015  0.28349453]\n",
      " [-0.20265419 -0.04393417  0.15789945 -0.00593344 -0.18382524]]\n",
      "\n",
      "correct h_next:\n",
      "[[ 0.05432073 -0.0240766  -0.08374854  0.00076676 -0.14279187]\n",
      " [-0.26776613  0.31730641 -0.21700223 -0.28390231 -0.26996079]\n",
      " [-0.13447024 -0.08660263 -0.10022437 -0.03162882 -0.05513277]\n",
      " [-0.07544577  0.17997456 -0.08875356  0.28115015  0.28349453]\n",
      " [-0.20265419 -0.04393417  0.15789945 -0.00593344 -0.18382524]]\n",
      "\n",
      "Difference between your h_next and correct h_next:  5.8426519119331166e-08\n"
     ]
    }
   ],
   "source": [
    "print('Your h_next:')\n",
    "print(h_next)\n",
    "print()\n",
    "print('correct h_next:')\n",
    "print(correct_h_next)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your h_next and correct h_next: ', np.sum(np.abs(h_next - correct_h_next)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your c_next:\n",
      "[[ 0.14534732 -0.12841532 -0.17270371  0.0028584  -0.33097237]\n",
      " [-0.40553132  0.52289938 -0.46666314 -0.41915372 -0.52302227]\n",
      " [-0.5563115  -0.20751267 -0.33762134 -0.14975814 -0.34766355]\n",
      " [-0.12754703  0.35234149 -0.1480342   0.517628    0.47883466]\n",
      " [-0.30773508 -0.14781391  0.21382212 -0.01451843 -0.31231354]]\n",
      "\n",
      "correct c_next:\n",
      "[[ 0.14534732 -0.12841532 -0.17270371  0.0028584  -0.33097237]\n",
      " [-0.40553132  0.52289938 -0.46666314 -0.41915372 -0.52302227]\n",
      " [-0.5563115  -0.20751267 -0.33762134 -0.14975814 -0.34766355]\n",
      " [-0.12754703  0.35234149 -0.1480342   0.517628    0.47883466]\n",
      " [-0.30773508 -0.14781391  0.21382212 -0.01451843 -0.31231354]]\n",
      "\n",
      "Difference between your c_next and correct c_next:  6.203414870198723e-08\n"
     ]
    }
   ],
   "source": [
    "print('Your c_next:')\n",
    "print(c_next)\n",
    "print()\n",
    "print('correct c_next:')\n",
    "print(correct_c_next)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your c_next and correct c_next: ', np.sum(np.abs(c_next - correct_c_next)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Dense import Dense\n",
    "from layers.TimeDistributed import TimeDistributed\n",
    "from utils.model_loss import td_cross_entropy_loss_npdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_timedist():\n",
    "    np.random.seed(0)\n",
    "    return TimeDistributed(Dense(dim_input=5, dim_output=2), out_size=2)\n",
    "\n",
    "def create_toy_sequence():\n",
    "    np.random.seed(1)\n",
    "    return np.random.uniform(-1, 1, (3, 4, 5)), np.array([[1, 1, 1, 0],\n",
    "                                                          [1, 1, 0, 1],\n",
    "                                                          [0, 1, 1, 1]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = create_toy_timedist()\n",
    "x, y = create_toy_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.03128610e-04,  1.66784694e-04],\n",
       "        [-2.83343210e-04, -1.36738569e-04],\n",
       "        [-2.10377603e-05,  8.40539508e-05],\n",
       "        [ 3.77615030e-06, -4.87965243e-05]],\n",
       "\n",
       "       [[ 1.56853284e-04,  2.95488224e-04],\n",
       "        [-1.84717967e-04, -2.32514097e-05],\n",
       "        [ 1.62175389e-05, -1.42246891e-04],\n",
       "        [-2.92006383e-07,  3.85406131e-05]],\n",
       "\n",
       "       [[ 2.02205524e-04,  1.51920385e-04],\n",
       "        [-4.81784643e-05,  1.95335290e-04],\n",
       "        [-2.86652305e-04,  1.04453634e-04],\n",
       "        [-2.62175136e-04,  8.02224474e-05]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = layer.forward_npdl(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dScores, softmax_output = td_cross_entropy_loss_npdl(z, y, 0.0, {'l1': layer.get_params()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.27273813e-05, -2.10320295e-05,  4.74051616e-05,\n",
       "          1.83540318e-05, -8.56203866e-06],\n",
       "        [ 2.27299193e-05, -2.10343783e-05,  4.74104556e-05,\n",
       "          1.83560815e-05, -8.56299483e-06],\n",
       "        [ 2.27303912e-05, -2.10348149e-05,  4.74114398e-05,\n",
       "          1.83564625e-05, -8.56317258e-06],\n",
       "        [-2.27309881e-05,  2.10353673e-05, -4.74126848e-05,\n",
       "         -1.83569445e-05,  8.56339746e-06]],\n",
       "\n",
       "       [[ 2.27300099e-05, -2.10344621e-05,  4.74106446e-05,\n",
       "          1.83561546e-05, -8.56302895e-06],\n",
       "        [ 2.27297504e-05, -2.10342220e-05,  4.74101033e-05,\n",
       "          1.83559450e-05, -8.56293119e-06],\n",
       "        [-2.27297846e-05,  2.10342535e-05, -4.74101745e-05,\n",
       "         -1.83559726e-05,  8.56294405e-06],\n",
       "        [ 2.27311443e-05, -2.10355118e-05,  4.74130106e-05,\n",
       "          1.83570707e-05, -8.56345629e-06]],\n",
       "\n",
       "       [[-2.27310141e-05,  2.10353914e-05, -4.74127391e-05,\n",
       "         -1.83569655e-05,  8.56340725e-06],\n",
       "        [ 2.27288179e-05, -2.10333590e-05,  4.74081582e-05,\n",
       "          1.83551920e-05, -8.56257988e-06],\n",
       "        [ 2.27271404e-05, -2.10318066e-05,  4.74046592e-05,\n",
       "          1.83538372e-05, -8.56194792e-06],\n",
       "        [ 2.27276940e-05, -2.10323189e-05,  4.74058140e-05,\n",
       "          1.83542843e-05, -8.56215648e-06]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX = layer.backward_npdl(dScores)\n",
    "dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Stanford Sentiment Treebank V1.0\n",
    "dictionary = pd.read_csv('datasets/stanfordSentimentTreebank/dictionary.txt', header=None, sep='|')\n",
    "dictionary = dictionary.rename(columns={0:'phrase', 1:'phrase_id'})\n",
    "\n",
    "dataset_split = pd.read_csv('datasets/stanfordSentimentTreebank/datasetSplit.txt', sep=',')\n",
    "\n",
    "dataset_sentences = pd.read_csv('datasets/stanfordSentimentTreebank/datasetSentences.txt', sep='\\t')\n",
    "\n",
    "dataset_labels = pd.read_csv('datasets/stanfordSentimentTreebank/sentiment_labels.txt', sep='|')\n",
    "dataset_labels = dataset_labels.rename(columns={'phrase ids':'phrase_id', 'sentiment values':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les phrase_id des sentence\n",
    "sentences_merged = dataset_sentences.merge(dictionary, left_on='sentence', right_on='phrase', how='left').drop(columns=['phrase'])\n",
    "\n",
    "# Retirer les sentence qui n'ont pas de phrase_id\n",
    "sentences_clean = sentences_merged[~sentences_merged.phrase_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenir les labels\n",
    "sentences_with_labels = sentences_clean.merge(dataset_labels, on='phrase_id', how='left').drop(columns=['phrase_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separation train - valid - test\n",
    "sentences_split = sentences_with_labels.merge(dataset_split, on='sentence_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pl\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "bpemb_en = BPEmb(lang=\"en\", dim=25, vs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "def call_embed(value, embedder, max_length):\n",
    "    emb = embedder.embed(value)\n",
    "    return np.pad(emb, ((0, max_length - emb.shape[0]), (0, 0)), 'constant', constant_values=(0))\n",
    "\n",
    "def get_longest(value, embedder):\n",
    "    emb = embedder.embed(value)\n",
    "    return emb.shape[0]\n",
    "\n",
    "def convert_sentiment(value):\n",
    "    if value <= 0.4:\n",
    "        return 0\n",
    "    if value <= 0.6:\n",
    "        return 1\n",
    "    return 2\n",
    "    \n",
    "\n",
    "sentences_split['len'] = sentences_split.apply(lambda x: get_longest(x['sentence'], bpemb_en), axis=1)\n",
    "\n",
    "max_len = sentences_split.len.max()\n",
    "print(max_len)\n",
    "\n",
    "sentences_split['embedding'] = sentences_split.apply(lambda x: call_embed(x['sentence'], bpemb_en, max_len), axis=1)\n",
    "sentences_split['sentiment_label'] = sentences_split.apply(lambda x: convert_sentiment(x['sentiment']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sentences_split[sentences_split.splitset_label == 1].drop(columns=['splitset_label'])\n",
    "valid = sentences_split[sentences_split.splitset_label == 2].drop(columns=['splitset_label'])\n",
    "test = sentences_split[sentences_split.splitset_label == 3].drop(columns=['splitset_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train.embedding.tolist())\n",
    "valid_data = np.array(valid.embedding.tolist())\n",
    "test_data = np.array(test.embedding.tolist())\n",
    "\n",
    "train_labels = np.array(train.sentiment_label.tolist())\n",
    "valid_labels = np.array(valid.sentiment_label.tolist())\n",
    "test_labels = np.array(test.sentiment_label.tolist())\n",
    "\n",
    "train_data = np.concatenate((train_data, test_data), axis=0)\n",
    "train_labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Model import Model\n",
    "from layers.LSTM import LSTM\n",
    "from layers.Dense import Dense\n",
    "from layers.Flatten import Flatten\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "\n",
    "def create_lstm_network():\n",
    "    model = Model()\n",
    "    \n",
    "    lstm1 = LSTM(58, 25, 50, weight_scale=None)\n",
    "    dense1 = Dense(50, 3, weight_scale=None)\n",
    "    \n",
    "    flatten = Flatten()\n",
    "    \n",
    "    model.add(lstm1)\n",
    "    model.add(flatten)\n",
    "    model.add(dense1)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 0 0 1 2 2 2 2 2 1 2 1 0 2 2 0 0 0 0 2 2 2 2 2 1 1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "model = create_lstm_network()\n",
    "predictions = model.predict(test_data[:32])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.078290409631384\n"
     ]
    }
   ],
   "source": [
    "scores = model.forward_npdl(train_data[:32])\n",
    "loss, dScores, softmax_output = model.calculate_loss(scores, train_labels[:32], 0.0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch 20 / 573) loss: 0.974579\n",
      "(batch 40 / 573) loss: 1.152193\n",
      "(batch 60 / 573) loss: 1.014861\n",
      "(batch 80 / 573) loss: 1.117595\n",
      "(batch 100 / 573) loss: 1.080216\n",
      "(batch 120 / 573) loss: 1.094187\n",
      "(batch 140 / 573) loss: 0.971715\n",
      "(batch 160 / 573) loss: 0.985145\n",
      "(batch 180 / 573) loss: 1.049306\n",
      "(batch 200 / 573) loss: 1.103454\n",
      "(batch 220 / 573) loss: 1.116618\n",
      "(batch 240 / 573) loss: 1.242435\n",
      "(batch 260 / 573) loss: 1.239236\n",
      "(batch 280 / 573) loss: 1.271869\n",
      "(batch 300 / 573) loss: 1.034407\n",
      "(batch 320 / 573) loss: 1.084623\n",
      "(batch 340 / 573) loss: 1.165341\n",
      "(batch 360 / 573) loss: 0.925095\n",
      "(batch 380 / 573) loss: 1.015803\n",
      "(batch 400 / 573) loss: 0.995433\n",
      "(batch 420 / 573) loss: 1.047951\n",
      "(batch 440 / 573) loss: 0.919563\n",
      "(batch 460 / 573) loss: 1.009250\n",
      "(batch 480 / 573) loss: 0.905819\n",
      "(batch 500 / 573) loss: 0.985285\n",
      "(batch 520 / 573) loss: 0.976965\n",
      "(batch 540 / 573) loss: 1.161027\n",
      "(batch 560 / 573) loss: 1.078440\n",
      "(epoch 1 / 10) loss: 1.039511, train_acc: 0.478660, val_acc: 0.505412\n",
      "(batch 20 / 573) loss: 1.105195\n",
      "(batch 40 / 573) loss: 1.047715\n",
      "(batch 60 / 573) loss: 1.054103\n",
      "(batch 80 / 573) loss: 0.714849\n",
      "(batch 100 / 573) loss: 1.085223\n",
      "(batch 120 / 573) loss: 1.053737\n",
      "(batch 140 / 573) loss: 1.137257\n",
      "(batch 160 / 573) loss: 1.102148\n",
      "(batch 180 / 573) loss: 0.891666\n",
      "(batch 200 / 573) loss: 1.003800\n",
      "(batch 220 / 573) loss: 1.118575\n",
      "(batch 240 / 573) loss: 1.020785\n",
      "(batch 260 / 573) loss: 0.824148\n",
      "(batch 280 / 573) loss: 0.971268\n",
      "(batch 300 / 573) loss: 1.094476\n",
      "(batch 320 / 573) loss: 0.829589\n",
      "(batch 340 / 573) loss: 1.111399\n",
      "(batch 360 / 573) loss: 1.095889\n",
      "(batch 380 / 573) loss: 0.898667\n",
      "(batch 400 / 573) loss: 0.932896\n",
      "(batch 420 / 573) loss: 0.887973\n",
      "(batch 440 / 573) loss: 0.881482\n",
      "(batch 460 / 573) loss: 0.869008\n",
      "(batch 480 / 573) loss: 0.994315\n",
      "(batch 500 / 573) loss: 0.948404\n",
      "(batch 520 / 573) loss: 0.922883\n",
      "(batch 540 / 573) loss: 1.081466\n",
      "(batch 560 / 573) loss: 0.928590\n",
      "(epoch 2 / 10) loss: 0.991452, train_acc: 0.561511, val_acc: 0.580235\n",
      "(batch 20 / 573) loss: 0.920178\n",
      "(batch 40 / 573) loss: 0.842038\n",
      "(batch 60 / 573) loss: 0.970641\n",
      "(batch 80 / 573) loss: 0.862688\n",
      "(batch 100 / 573) loss: 0.968430\n",
      "(batch 120 / 573) loss: 0.915735\n",
      "(batch 140 / 573) loss: 1.092521\n",
      "(batch 160 / 573) loss: 0.818701\n",
      "(batch 180 / 573) loss: 0.797499\n",
      "(batch 200 / 573) loss: 0.960930\n",
      "(batch 220 / 573) loss: 1.212721\n",
      "(batch 240 / 573) loss: 1.243187\n",
      "(batch 260 / 573) loss: 1.117101\n",
      "(batch 280 / 573) loss: 1.054559\n",
      "(batch 300 / 573) loss: 1.184558\n",
      "(batch 320 / 573) loss: 0.968806\n",
      "(batch 340 / 573) loss: 1.082316\n",
      "(batch 360 / 573) loss: 0.765474\n",
      "(batch 380 / 573) loss: 0.992946\n",
      "(batch 400 / 573) loss: 0.847539\n",
      "(batch 420 / 573) loss: 0.967809\n",
      "(batch 440 / 573) loss: 0.820682\n",
      "(batch 460 / 573) loss: 0.727446\n",
      "(batch 480 / 573) loss: 1.115065\n",
      "(batch 500 / 573) loss: 0.725157\n",
      "(batch 520 / 573) loss: 0.798417\n",
      "(batch 540 / 573) loss: 0.728456\n",
      "(batch 560 / 573) loss: 0.821514\n",
      "(epoch 3 / 10) loss: 0.964059, train_acc: 0.571117, val_acc: 0.577882\n",
      "(batch 20 / 573) loss: 0.780153\n",
      "(batch 40 / 573) loss: 0.949984\n",
      "(batch 60 / 573) loss: 0.960544\n",
      "(batch 80 / 573) loss: 0.973278\n",
      "(batch 100 / 573) loss: 1.238580\n",
      "(batch 120 / 573) loss: 1.028034\n",
      "(batch 140 / 573) loss: 0.931102\n",
      "(batch 160 / 573) loss: 0.970226\n",
      "(batch 180 / 573) loss: 1.042843\n",
      "(batch 200 / 573) loss: 1.132432\n",
      "(batch 220 / 573) loss: 0.771328\n",
      "(batch 240 / 573) loss: 0.780909\n",
      "(batch 260 / 573) loss: 0.975995\n",
      "(batch 280 / 573) loss: 0.927209\n",
      "(batch 300 / 573) loss: 1.080852\n",
      "(batch 320 / 573) loss: 0.820224\n",
      "(batch 340 / 573) loss: 1.080147\n",
      "(batch 360 / 573) loss: 1.034300\n",
      "(batch 380 / 573) loss: 0.850846\n",
      "(batch 400 / 573) loss: 0.770475\n",
      "(batch 420 / 573) loss: 1.021861\n",
      "(batch 440 / 573) loss: 1.088676\n",
      "(batch 460 / 573) loss: 0.814570\n",
      "(batch 480 / 573) loss: 0.890120\n",
      "(batch 500 / 573) loss: 0.924331\n",
      "(batch 520 / 573) loss: 0.956345\n",
      "(batch 540 / 573) loss: 1.264582\n",
      "(batch 560 / 573) loss: 1.062964\n",
      "(epoch 4 / 10) loss: 0.951812, train_acc: 0.592512, val_acc: 0.602824\n",
      "(batch 20 / 573) loss: 0.872379\n",
      "(batch 40 / 573) loss: 1.199602\n",
      "(batch 60 / 573) loss: 0.833901\n",
      "(batch 80 / 573) loss: 1.046765\n",
      "(batch 100 / 573) loss: 1.170335\n",
      "(batch 120 / 573) loss: 1.013817\n",
      "(batch 140 / 573) loss: 1.080847\n",
      "(batch 160 / 573) loss: 1.132317\n",
      "(batch 180 / 573) loss: 0.768875\n",
      "(batch 200 / 573) loss: 0.834967\n",
      "(batch 220 / 573) loss: 1.018779\n",
      "(batch 240 / 573) loss: 0.796012\n",
      "(batch 260 / 573) loss: 0.871615\n",
      "(batch 280 / 573) loss: 0.969900\n",
      "(batch 300 / 573) loss: 1.069244\n",
      "(batch 320 / 573) loss: 1.146113\n",
      "(batch 340 / 573) loss: 0.785022\n",
      "(batch 360 / 573) loss: 0.976108\n",
      "(batch 380 / 573) loss: 0.777718\n",
      "(batch 400 / 573) loss: 0.914625\n",
      "(batch 420 / 573) loss: 0.895629\n",
      "(batch 440 / 573) loss: 0.853667\n",
      "(batch 460 / 573) loss: 0.966493\n",
      "(batch 480 / 573) loss: 0.957006\n",
      "(batch 500 / 573) loss: 0.948990\n",
      "(batch 520 / 573) loss: 0.954563\n",
      "(batch 540 / 573) loss: 0.836208\n",
      "(batch 560 / 573) loss: 0.763146\n",
      "(epoch 5 / 10) loss: 0.937496, train_acc: 0.601681, val_acc: 0.604235\n",
      "(batch 20 / 573) loss: 1.002257\n",
      "(batch 40 / 573) loss: 0.828638\n",
      "(batch 60 / 573) loss: 0.524930\n",
      "(batch 80 / 573) loss: 0.939630\n",
      "(batch 100 / 573) loss: 0.758924\n",
      "(batch 120 / 573) loss: 0.964489\n",
      "(batch 140 / 573) loss: 0.887948\n",
      "(batch 160 / 573) loss: 0.772055\n",
      "(batch 180 / 573) loss: 0.733915\n",
      "(batch 200 / 573) loss: 1.076244\n",
      "(batch 220 / 573) loss: 1.043716\n",
      "(batch 240 / 573) loss: 0.711901\n",
      "(batch 260 / 573) loss: 1.115141\n",
      "(batch 280 / 573) loss: 0.884066\n",
      "(batch 300 / 573) loss: 0.871541\n",
      "(batch 320 / 573) loss: 0.683153\n",
      "(batch 340 / 573) loss: 0.899542\n",
      "(batch 360 / 573) loss: 1.098689\n",
      "(batch 380 / 573) loss: 0.973744\n",
      "(batch 400 / 573) loss: 0.891085\n",
      "(batch 420 / 573) loss: 0.821936\n",
      "(batch 440 / 573) loss: 0.797638\n",
      "(batch 460 / 573) loss: 0.910148\n",
      "(batch 480 / 573) loss: 0.657517\n",
      "(batch 500 / 573) loss: 0.930290\n",
      "(batch 520 / 573) loss: 0.742886\n",
      "(batch 540 / 573) loss: 1.057855\n",
      "(batch 560 / 573) loss: 1.361930\n",
      "(epoch 6 / 10) loss: 0.926878, train_acc: 0.612160, val_acc: 0.609412\n",
      "(batch 20 / 573) loss: 0.994089\n",
      "(batch 40 / 573) loss: 0.775461\n",
      "(batch 60 / 573) loss: 0.801794\n",
      "(batch 80 / 573) loss: 0.919476\n",
      "(batch 100 / 573) loss: 0.737679\n",
      "(batch 120 / 573) loss: 0.822729\n",
      "(batch 140 / 573) loss: 0.920036\n",
      "(batch 160 / 573) loss: 0.900508\n",
      "(batch 180 / 573) loss: 1.015562\n",
      "(batch 200 / 573) loss: 0.852077\n",
      "(batch 220 / 573) loss: 1.082901\n",
      "(batch 240 / 573) loss: 0.857573\n",
      "(batch 260 / 573) loss: 1.037656\n",
      "(batch 280 / 573) loss: 0.956298\n",
      "(batch 300 / 573) loss: 0.961043\n",
      "(batch 320 / 573) loss: 0.793038\n",
      "(batch 340 / 573) loss: 0.637762\n",
      "(batch 360 / 573) loss: 0.853674\n",
      "(batch 380 / 573) loss: 0.777098\n",
      "(batch 400 / 573) loss: 0.709234\n",
      "(batch 420 / 573) loss: 0.813015\n",
      "(batch 440 / 573) loss: 0.908839\n",
      "(batch 460 / 573) loss: 0.948869\n",
      "(batch 480 / 573) loss: 0.941233\n",
      "(batch 500 / 573) loss: 0.849590\n",
      "(batch 520 / 573) loss: 0.912919\n",
      "(batch 540 / 573) loss: 1.002386\n",
      "(batch 560 / 573) loss: 1.014524\n",
      "(epoch 7 / 10) loss: 0.908033, train_acc: 0.617727, val_acc: 0.614588\n",
      "(batch 20 / 573) loss: 0.818586\n",
      "(batch 40 / 573) loss: 1.002079\n",
      "(batch 60 / 573) loss: 0.906387\n",
      "(batch 80 / 573) loss: 0.873632\n",
      "(batch 100 / 573) loss: 0.927558\n",
      "(batch 120 / 573) loss: 0.663920\n",
      "(batch 140 / 573) loss: 0.827075\n",
      "(batch 160 / 573) loss: 0.866953\n",
      "(batch 180 / 573) loss: 0.940588\n",
      "(batch 200 / 573) loss: 0.871924\n",
      "(batch 220 / 573) loss: 0.900542\n",
      "(batch 240 / 573) loss: 1.161595\n",
      "(batch 260 / 573) loss: 1.010530\n",
      "(batch 280 / 573) loss: 0.890823\n",
      "(batch 300 / 573) loss: 0.672504\n",
      "(batch 320 / 573) loss: 1.250440\n",
      "(batch 340 / 573) loss: 0.538382\n",
      "(batch 360 / 573) loss: 1.153982\n",
      "(batch 380 / 573) loss: 0.721252\n",
      "(batch 400 / 573) loss: 0.874466\n",
      "(batch 420 / 573) loss: 0.982503\n",
      "(batch 440 / 573) loss: 0.851476\n",
      "(batch 460 / 573) loss: 0.926560\n",
      "(batch 480 / 573) loss: 1.187946\n",
      "(batch 500 / 573) loss: 1.023955\n",
      "(batch 520 / 573) loss: 0.749342\n",
      "(batch 540 / 573) loss: 0.966206\n",
      "(batch 560 / 573) loss: 0.679749\n",
      "(epoch 8 / 10) loss: 0.898585, train_acc: 0.627006, val_acc: 0.613647\n",
      "(batch 20 / 573) loss: 0.841911\n",
      "(batch 40 / 573) loss: 0.973509\n",
      "(batch 60 / 573) loss: 1.144802\n",
      "(batch 80 / 573) loss: 1.025378\n",
      "(batch 100 / 573) loss: 0.793590\n",
      "(batch 120 / 573) loss: 0.979660\n",
      "(batch 140 / 573) loss: 1.003568\n",
      "(batch 160 / 573) loss: 0.853815\n",
      "(batch 180 / 573) loss: 0.994347\n",
      "(batch 200 / 573) loss: 1.056892\n",
      "(batch 220 / 573) loss: 1.282798\n",
      "(batch 240 / 573) loss: 0.901461\n",
      "(batch 260 / 573) loss: 1.013892\n",
      "(batch 280 / 573) loss: 0.636670\n",
      "(batch 300 / 573) loss: 0.885254\n",
      "(batch 320 / 573) loss: 0.808980\n",
      "(batch 340 / 573) loss: 0.851938\n",
      "(batch 360 / 573) loss: 0.789001\n",
      "(batch 380 / 573) loss: 0.873476\n",
      "(batch 400 / 573) loss: 0.965703\n",
      "(batch 420 / 573) loss: 0.778849\n",
      "(batch 440 / 573) loss: 0.816703\n",
      "(batch 460 / 573) loss: 1.118528\n",
      "(batch 480 / 573) loss: 0.742370\n",
      "(batch 500 / 573) loss: 0.800299\n",
      "(batch 520 / 573) loss: 1.248756\n",
      "(batch 540 / 573) loss: 0.886264\n",
      "(batch 560 / 573) loss: 0.865502\n",
      "(epoch 9 / 10) loss: 0.886048, train_acc: 0.634429, val_acc: 0.617412\n",
      "(batch 20 / 573) loss: 0.731827\n",
      "(batch 40 / 573) loss: 0.920997\n",
      "(batch 60 / 573) loss: 0.879499\n",
      "(batch 80 / 573) loss: 1.122389\n",
      "(batch 100 / 573) loss: 0.986471\n",
      "(batch 120 / 573) loss: 0.566580\n",
      "(batch 140 / 573) loss: 0.758857\n",
      "(batch 160 / 573) loss: 0.740524\n",
      "(batch 180 / 573) loss: 0.820086\n",
      "(batch 200 / 573) loss: 0.994771\n",
      "(batch 220 / 573) loss: 0.877845\n",
      "(batch 240 / 573) loss: 0.946632\n",
      "(batch 260 / 573) loss: 0.873061\n",
      "(batch 280 / 573) loss: 0.667926\n",
      "(batch 300 / 573) loss: 0.705418\n",
      "(batch 320 / 573) loss: 0.938417\n",
      "(batch 340 / 573) loss: 0.914092\n",
      "(batch 360 / 573) loss: 0.715442\n",
      "(batch 380 / 573) loss: 1.414134\n",
      "(batch 400 / 573) loss: 0.909374\n",
      "(batch 420 / 573) loss: 1.082898\n",
      "(batch 440 / 573) loss: 1.029756\n",
      "(batch 460 / 573) loss: 0.761273\n",
      "(batch 480 / 573) loss: 0.812059\n",
      "(batch 500 / 573) loss: 0.621959\n",
      "(batch 520 / 573) loss: 0.851066\n",
      "(batch 540 / 573) loss: 0.749477\n",
      "(batch 560 / 573) loss: 0.810865\n",
      "(epoch 10 / 10) loss: 0.874938, train_acc: 0.627879, val_acc: 0.611294\n"
     ]
    }
   ],
   "source": [
    "from model.Solver import epoch_solver_npdl, Adam, SGD\n",
    "\n",
    "model = create_lstm_network()\n",
    "\n",
    "optimizer = Adam(1e-3, model)\n",
    "    \n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_npdl(train_data, \n",
    "                                                                          train_labels,\n",
    "                                                                          valid_data,\n",
    "                                                                          valid_labels,\n",
    "                                                                          2e-4,\n",
    "                                                                          optimizer,\n",
    "                                                                          lr_decay=0.95,\n",
    "                                                                          batch_size=16,\n",
    "                                                                          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
