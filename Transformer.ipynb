{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.activations import softmax_forward_npdl, softmax_backward_npdl\n",
    "from layers.Attention import SelfAttention, MultiHeadAttention\n",
    "from utils.model_loss import cross_entropy_loss_npdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[[0.1,0.2,0.1,0.5], [0,0,0,0], [0.1,0.2,0.3,0.2]], [[0.2,0.3,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.2,0.3,0.2]], [[0,0,0,0], [0.2,0.1,0.1,1.5], [0.1,0.2,0.3,0.2]]])\n",
    "K = np.array([[[0.1,0.2,0.1,0.5], [0,0,0,0], [0.1,0.2,0.3,0.2]], [[0.2,0.3,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.2,0.3,0.2]], [[0,0,0,0], [0.2,0.1,0.1,1.5], [0.1,0.2,0.3,0.2]]])\n",
    "V = np.array([[[0.1,0.2,0.1,0.5], [0,0,0,0], [0.1,0.2,0.3,0.2]], [[0.2,0.3,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.2,0.3,0.2]], [[0,0,0,0], [0.2,0.1,0.1,1.5], [0.1,0.2,0.3,0.2]]]) \n",
    "\n",
    "mask = np.array([[[1],[0],[1]],[[1],[1],[1]],[[0],[1],[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "layer = SelfAttention(4, 3, 3, weight_scale=None)\n",
    "np.random.seed(0)\n",
    "A = layer.forward_npdl(Q, K, V, output_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "dA = np.random.randn(3, 3, 3)\n",
    "np.random.seed(0)\n",
    "dQ_p, dK_p, dV_p = layer.backward_npdl(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.dWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.WV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "multi = MultiHeadAttention(4, 4, weight_scale=None)\n",
    "a = np.random.randn(3,2,4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = multi.forward_npdl(a, a, a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multi.backward_npdl(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[0.1,0.2,0.1,0.5], [0,0,0,0], [0.1,0.2,0.3,0.2]], [[0.2,0.3,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.2,0.3,0.2]], [[0,0,0,0], [0.2,0.1,0.1,1.5], [0.1,0.2,0.3,0.2]]])\n",
    "mask = np.array([[[1],[0],[1]],[[1],[1],[1]],[[0],[1],[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = encoder.forward_npdl(X, output_mask=mask)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = np.random.rand(*(A.shape))\n",
    "dX = encoder.backward_npdl(dA, output_mask=mask)\n",
    "print(dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Transformer import TransformerEncoder\n",
    "from utils.model_loss import cross_entropy_loss_npdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[0.1,0.2,0.1,0.5], [0,0,0,0], [0.1,0.2,0.3,0.2]], [[0.2,0.3,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.2,0.3,0.2]], [[0,0,0,0], [0.2,0.1,0.1,1.5], [0.1,0.2,0.3,0.2]]])\n",
    "t = np.array([1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(2, 3, 4, 8, 3)\n",
    "model.add_loss(cross_entropy_loss_npdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.forward_npdl(X)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = np.random.rand(*A.shape)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = model.backward_npdl(dA)\n",
    "print(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dScores, softmax_output = model.calculate_loss(A, t, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Stanford Sentiment Treebank V1.0\n",
    "dictionary = pd.read_csv('datasets/stanfordSentimentTreebank/dictionary.txt', header=None, sep='|')\n",
    "dictionary = dictionary.rename(columns={0:'phrase', 1:'phrase_id'})\n",
    "\n",
    "dataset_split = pd.read_csv('datasets/stanfordSentimentTreebank/datasetSplit.txt', sep=',')\n",
    "\n",
    "dataset_sentences = pd.read_csv('datasets/stanfordSentimentTreebank/datasetSentences.txt', sep='\\t')\n",
    "\n",
    "dataset_labels = pd.read_csv('datasets/stanfordSentimentTreebank/sentiment_labels.txt', sep='|')\n",
    "dataset_labels = dataset_labels.rename(columns={'phrase ids':'phrase_id', 'sentiment values':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les phrase_id des sentence\n",
    "sentences_merged = dataset_sentences.merge(dictionary, left_on='sentence', right_on='phrase', how='left').drop(columns=['phrase'])\n",
    "\n",
    "# Retirer les sentence qui n'ont pas de phrase_id\n",
    "sentences_clean = sentences_merged[~sentences_merged.phrase_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenir les labels\n",
    "sentences_with_labels = sentences_clean.merge(dataset_labels, on='phrase_id', how='left').drop(columns=['phrase_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separation train - valid - test\n",
    "sentences_split = sentences_with_labels.merge(dataset_split, on='sentence_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_en = BPEmb(lang=\"en\", dim=50, vs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_embed(value, embedder, max_length):\n",
    "    emb = embedder.embed(value)\n",
    "    return np.pad(emb, ((0, max_length - emb.shape[0]), (0, 0)), 'constant', constant_values=(0))\n",
    "\n",
    "def get_longest(value, embedder):\n",
    "    emb = embedder.embed(value)\n",
    "    return emb.shape[0]\n",
    "\n",
    "def convert_sentiment(value):\n",
    "    if value <= 0.4:\n",
    "        return 0\n",
    "    if value <= 0.6:\n",
    "        return 1\n",
    "    return 2\n",
    "    \n",
    "\n",
    "sentences_split['len'] = sentences_split.apply(lambda x: get_longest(x['sentence'], bpemb_en), axis=1)\n",
    "\n",
    "max_len = sentences_split.len.max()\n",
    "print(max_len)\n",
    "\n",
    "sentences_split['embedding'] = sentences_split.apply(lambda x: call_embed(x['sentence'], bpemb_en, max_len), axis=1)\n",
    "sentences_split['sentiment_label'] = sentences_split.apply(lambda x: convert_sentiment(x['sentiment']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sentences_split[sentences_split.splitset_label == 1].drop(columns=['splitset_label'])\n",
    "valid = sentences_split[sentences_split.splitset_label == 2].drop(columns=['splitset_label'])\n",
    "test = sentences_split[sentences_split.splitset_label == 3].drop(columns=['splitset_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train.embedding.tolist())\n",
    "valid_data = np.array(valid.embedding.tolist())\n",
    "test_data = np.array(test.embedding.tolist())\n",
    "\n",
    "train_labels = np.array(train.sentiment_label.tolist())\n",
    "valid_labels = np.array(valid.sentiment_label.tolist())\n",
    "test_labels = np.array(test.sentiment_label.tolist())\n",
    "\n",
    "train_data = np.concatenate((train_data, test_data), axis=0)\n",
    "train_labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Transformer import TransformerEncoder\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "from model.Solver import check_accuracy\n",
    "\n",
    "def create_transformer_network():\n",
    "    model = TransformerEncoder(2, 58, 50, 100, 3, num_heads=5)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_transformer_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[:32])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.forward_npdl(train_data[:32])\n",
    "loss, dScores, softmax_output = model.calculate_loss(scores, train_labels[:32], 0.0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = check_accuracy(train_data, train_labels, 16, model)\n",
    "val_accuracy = check_accuracy(valid_data, valid_labels, 16, model)\n",
    "print('Initial training accuracy: ' + str(train_accuracy))\n",
    "print('Initial validation accuracy: ' + str(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Solver import epoch_solver_npdl, Adam, SGD\n",
    "\n",
    "model = create_transformer_network()\n",
    "\n",
    "optimizer = Adam(1e-3, model)\n",
    "\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_npdl(train_data, \n",
    "                                                                          train_labels,\n",
    "                                                                          valid_data,\n",
    "                                                                          valid_labels,\n",
    "                                                                          2e-3,\n",
    "                                                                          optimizer,\n",
    "                                                                          lr_decay=0.99,\n",
    "                                                                          batch_size=16,\n",
    "                                                                          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bpemb import BPEmb\n",
    "from model.Solver import epoch_solver_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Twitter customer support\n",
    "dataset = pd.read_csv('datasets/twcs/twcs.csv').drop(columns=['author_id', 'inbound', 'created_at', 'in_response_to_tweet_id'])\n",
    "dataset = dataset.set_index('tweet_id')\n",
    "dataset_responses = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets with NaN response_tweet_id from dataset\n",
    "dataset = dataset[~dataset.response_tweet_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If multiple responses to one tweet, only keep the first\n",
    "def split_fct(x):\n",
    "    if ',' in str(x):\n",
    "        return int(str(x).split(',')[0])\n",
    "    \n",
    "    return int(x)\n",
    "\n",
    "dataset['response_tweet_id'] = dataset['response_tweet_id'].apply(lambda x: split_fct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge responses with questions\n",
    "pair_dataset = dataset.merge(dataset_responses, left_on='response_tweet_id', right_on='tweet_id', how='left').drop(columns=['response_tweet_id_x', 'response_tweet_id_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove samples with null text\n",
    "pair_dataset = pair_dataset[~pair_dataset.text_x.isnull()]\n",
    "pair_dataset = pair_dataset[~pair_dataset.text_y.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove @ mentions from text\n",
    "def remove_mentions(x):\n",
    "    return ' '.join(filter(lambda z: z[0] != '@', x.split()))\n",
    "\n",
    "pair_dataset['text_x'] = pair_dataset['text_x'].apply(lambda x: remove_mentions(x))\n",
    "pair_dataset['text_y'] = pair_dataset['text_y'].apply(lambda x: remove_mentions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size to 5k (for speed purposes)\n",
    "pair_dataset = pair_dataset[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_en = BPEmb(lang=\"en\", dim=25, vs=25000, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_embed(value, embedder, max_length):\n",
    "    emb = embedder.embed(value)\n",
    "    return np.pad(emb, ((0, max_length - emb.shape[0]), (0, 0)), 'constant', constant_values=(0))\n",
    "\n",
    "def call_embed_target(value, embedder, max_length):\n",
    "    ids = embedder.encode_ids_with_bos_eos(value)\n",
    "    emb = embedder.emb.vectors[ids]\n",
    "    return np.pad(emb, ((0, max_length - len(emb)), (0, 0)), 'constant', constant_values=(0))\n",
    "\n",
    "def tokenize(value, embedder, max_length):\n",
    "    tokens = bpemb_en.encode_ids_with_eos(value)\n",
    "    return np.pad(tokens, (0, max_length - len(tokens)), 'constant', constant_values=(1)).astype(int)\n",
    "\n",
    "def get_longest(value, embedder):\n",
    "    emb = embedder.embed(value)\n",
    "    return emb.shape[0]\n",
    "\n",
    "def get_longest_target(value, embedder):\n",
    "    ids = embedder.encode_ids_with_bos_eos(value)\n",
    "    emb = embedder.emb.vectors[ids]\n",
    "    return emb.shape[0]\n",
    "    \n",
    "\n",
    "pair_dataset['len_x'] = pair_dataset.apply(lambda x: get_longest(x['text_x'], bpemb_en), axis=1)\n",
    "pair_dataset['len_y'] = pair_dataset.apply(lambda x: get_longest_target(x['text_y'], bpemb_en), axis=1)\n",
    "\n",
    "max_len = max(pair_dataset.len_x.max(), pair_dataset.len_y.max())\n",
    "print(max_len)\n",
    "\n",
    "pair_dataset['embedding_x'] = pair_dataset.apply(lambda x: call_embed(x['text_x'], bpemb_en, max_len), axis=1)\n",
    "pair_dataset['embedding_y'] = pair_dataset.apply(lambda x: call_embed_target(x['text_y'], bpemb_en, max_len), axis=1)\n",
    "\n",
    "pair_dataset['token_id_y'] = pair_dataset.apply(lambda x: tokenize(x['text_y'], bpemb_en, max_len), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pair_dataset[:4000]\n",
    "valid = pair_dataset[4000:5000]\n",
    "\n",
    "train_data = np.array(train.embedding_x.tolist())\n",
    "valid_data = np.array(valid.embedding_x.tolist())\n",
    "\n",
    "train_targets = np.array(train.embedding_y.tolist())\n",
    "valid_targets = np.array(valid.embedding_y.tolist())\n",
    "\n",
    "train_labels = np.array(train.token_id_y.tolist())\n",
    "valid_labels = np.array(valid.token_id_y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pair_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Transformer import Transformer\n",
    "from utils.model_loss import td_cross_entropy_loss_npdl\n",
    "\n",
    "def create_transformer_network():\n",
    "    model = Transformer(2, 99, 25, 50, 25000, num_heads=5)\n",
    "    model.add_loss(td_cross_entropy_loss_npdl)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_transformer_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.forward_npdl(train_data[:2], train_targets[:2])\n",
    "loss, dScores, softmax_output = model.calculate_loss(scores, train_labels[:2], 0.0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dScores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA, dE = model.backward_npdl(dScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Solver import epoch_solver_seq2seq, Adam, SGD\n",
    "\n",
    "model = create_transformer_network()\n",
    "\n",
    "optimizer = Adam(1e-3, model)\n",
    "\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_seq2seq(train_data,\n",
    "                                                                                  train_targets,\n",
    "                                                                                  train_labels,\n",
    "                                                                                  valid_data,\n",
    "                                                                                  valid_labels,\n",
    "                                                                                  0.0,\n",
    "                                                                                  optimizer,\n",
    "                                                                                  lr_decay=0.99,\n",
    "                                                                                  batch_size=16,\n",
    "                                                                                  epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.utils import visualize_loss\n",
    "visualize_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On voit que l'entraînement aurait encore beaucoup de chemin à faire et que le modèle\n",
    "# n'a probablement pas assez de capacité pour cette tâche.\n",
    "scores = model.forward_npdl(train_data[1].reshape(1, 99, 25), train_targets[1].reshape(1, 99, 25))\n",
    "np.argmax(scores, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_data[1].reshape(1, 99, 25), bpemb_en, 1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
